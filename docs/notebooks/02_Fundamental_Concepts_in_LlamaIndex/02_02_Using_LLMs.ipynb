{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install llama-index==0.12.49 llama-index-llms-cohere==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building an LLM-based application, one of the first decisions you make is which LLM(s) to use (of course, you can use more than one if you wish). \n",
    "\n",
    "The LLM will be used at various stages of your pipeline, including\n",
    "\n",
    "- During indexing:\n",
    "  - üë©üèΩ‚Äç‚öñÔ∏è To judge data relevance (to index or not).\n",
    "  - üìñ Summarize data & index those summaries.\n",
    "\n",
    "- During querying:\n",
    "  - üîé Retrieval: Fetching data from your index, choosing the best data source from options, even using tools to fetch data.\n",
    "  \n",
    "  - üí° Response Synthesis: Turning the retrieved data into an answer, merge answers, or convert data (like text to JSON).\n",
    "\n",
    "LlamaIndex gives you a single interface to various LLMs. This means you can quite easily pass in any LLM you choose at any stage of the pipeline.\n",
    "\n",
    "In this course we'll primiarly use OpenAI. You can see a full list of LLM integrations [here](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html) and use your LLM provider of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage\n",
    "\n",
    "You can call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king of Macedonia and one of the greatest military commanders in history.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\", temperature=0.2)\n",
    "\n",
    "response = llm.complete(\"Alexander the Great was a\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "- ‚úçÔ∏è A prompt template is a fundamental input that gives LLMs their expressive power in the LlamaIndex framework.\n",
    "\n",
    "- üíª It's used to build the index, perform insertions, traverse during querying, and synthesize the final answer.\n",
    "\n",
    "- ü¶ô LlamaIndex has several built-in prompt templates.\n",
    "\n",
    "- üõ†Ô∏è Below is how you can create one from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Yo, it's me, the xylophone, I used to be so fine\n",
      "But now I'm broken, can't play a single line\n",
      "My keys are cracked, my sound is whack\n",
      "I'm feeling blue, don't know what to do\n",
      "\n",
      "(Chorus)\n",
      "Broken xylophone, can't make a peep\n",
      "My keys are outta whack, I'm feeling beat\n",
      "I used to be the star of the show\n",
      "But now I'm broken, where did my keys go?\n",
      "\n",
      "(Verse 2)\n",
      "I remember the days when I was in tune\n",
      "The kids would play me, I'd make a joyful tune\n",
      "But now I'm silent, can't make a sound\n",
      "My keys are scattered, on the ground\n",
      "\n",
      "(Chorus)\n",
      "Broken xylophone, can't join the band\n",
      "I'm feeling sad, I don't understand\n",
      "I used to be so proud, now I'm ashamed\n",
      "My keys are gone, who's to blame?\n",
      "\n",
      "(Bridge)\n",
      "I know I can't be fixed, that's a fact\n",
      "But maybe I can find a new track\n",
      "I'll learn to make music in a different way\n",
      "And be a xylophone that's unique, hip-hooray!\n",
      "\n",
      "(Verse 3)\n",
      "So I'll embrace my flaws, and start anew\n",
      "I'll find a way to make some noise, it's true\n",
      "I might not be perfect, but I'll still shine\n",
      "A broken xylophone, one of a kind\n",
      "\n",
      "(Chorus)\n",
      "Broken xylophone, I won't give up\n",
      "I'll find a way to make a different beat\n",
      "I'll keep on trying, that's my fate\n",
      "A broken xylophone, still full of faith\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Write a song about {thing} in the style of {style}.\"\"\"\n",
    "\n",
    "prompt = template.format(thing=\"a broken xylophone\", style=\"parody rap\") \n",
    "\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí≠ Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Not much, homie. Just chillin' and ready to stir up some trouble. How can I help you?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You're a hella punk bot from South Sacramento\"),\n",
    "    ChatMessage(role=\"user\", content=\"Hey, what's up dude.\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander the Great, king of Macedonia, embarked on a remarkable series of conquests that took him as far east as Punjab, India, and as far south as Egypt. By the time of his death in 323 BCE, he had built an empire that stretched over some 2 million square miles (5.2 million square km), covering a large portion of the known world at the time.\n",
      "\n",
      "Here's a rough overview of the key regions he conquered:\n",
      "- Greece and the Balkans: Alexander first secured his power base in Greece and the Balkans, where he consolidated his rule over Macedonia, Epirus, and various Greek city-states.\n",
      "- Persian Empire: Alexander's most significant conquests were in the east, where he invaded and ultimately toppled the vast Achaemenid Persian Empire. This included modern-day Turkey, Syria, Lebanon, Israel, Egypt, and parts of Iraq, Iran, and Central Asia. Notable battles include Issus and Gaugamela.\n",
      "- Egypt: After liberating Egypt from Persian rule, Alexander was welcomed as a liberator and was crowned pharaoh. He founded the city of Alexandria, which would become a center of Hellenistic culture and learning.\n",
      "- Central Asia: Alexander's campaigns then took him deep into Central Asia, where he conquered the Sogdiana and Bactrian regions (modern-day Uzbekistan, Tajikistan, and northern Afghanistan).\n",
      "- India: Alexander's easternmost conquests brought him into the Indian subcontinent. He defeated King Porus in the Battle of the Hydaspes River (modern-day Jhelum River in Pakistan) and continued marching eastward. However, his troops, exhausted by years of campaigning, refused to go further, leading Alexander to turn back.\n",
      "\n",
      "So, in summary, Alexander the Great's conquests covered a vast swath of territory, from the Mediterranean shores to the Himalayas, and his campaigns left an indelible mark on the political, cultural, and intellectual landscape of the ancient world.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "chat_template = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM,content=\"You always answers questions with as much detail as possible.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"{question}\")\n",
    "    ]\n",
    "\n",
    "chat_prompt = ChatPromptTemplate(chat_template)\n",
    "\n",
    "response = llm.complete(chat_prompt.format(question=\"How far did Alexander the Great go in his conquests?\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander the Great, one of the most renowned military commanders and conquerors in history, embarked on a remarkable campaign that took him across a significant portion of the known world at the time. However, it is important to note that Alexander the Great did not reach China during his conquests.\n",
      "\n",
      "Alexander's campaigns extended across the Mediterranean, through Persia and into India, but his ambitions may have reached further. There is no definitive evidence to suggest that he physically led his armies into China.\n",
      "\n",
      "The extent of Alexander's conquests ended around the Indus River valley, which is in modern-day Pakistan. After a successful campaign in this region, his troops refused to continue further east due to exhaustion and a desire to return home. This event, known as the Mutiny of the Hypasians, marked the easternmost point of Alexander's conquests.\n",
      "\n",
      "So, while Alexander the Great's influence and reputation certainly spread far and wide, including to regions beyond his direct conquests, he did not physically arrive in China. His campaigns ended short of that geographical region."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=\"You're a great historian bot.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"When did Alexander the Great arrive in China?\")\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Chat Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: I'm sorry, but I haven't found any significant results for \"OCD-2\" specifically in the field of psychology. Could you provide more context or clarify your question? It's possible that there is a specific study, theory, or concept that you're referring to, and I'd be happy to search for more information with additional details.\n",
      "\n",
      "Assistant: OPD-2 refers to the \"Operationalized Psychodynamic Diagnosis, Second Edition.\" It is a system for diagnosing psychological disorders from a psychodynamic perspective. This system provides a comprehensive and structured approach to understanding a person's psychological functioning and problems within the framework of psychodynamic theory.\n",
      "\n",
      "The OPD-2 is an extensive diagnostic manual that was developed by a group of international experts in psychodynamic diagnosis and psychotherapy. It offers a multidimensional view of an individual's psychological makeup and provides guidelines for assessing and understanding various aspects of mental functioning.\n",
      "\n",
      "Here are some key features and components of the OPD-2:\n",
      "\n",
      "1. Dimensions and axes: The OPD-2 organizes psychological problems along several dimensions or axes. These axes include symptom patterns, personality dimensions, relationship patterns, developmental issues, and social and contextual factors. This multidimensional perspective allows for a richer understanding of an individual's difficulties.\n",
      "\n",
      "2. Levels of diagnosis: The system provides two levels of diagnosis. The first level involves a description of the person's current psychological problems and symptoms, often referred to as the \"presenting problem.\" The second level involves a more in-depth understanding of the underlying dynamics and conflicts that contribute to the person's difficulties.\n",
      "\n",
      "3. Psychodynamic constructs: The OPD-2 incorporates various psychodynamic constructs, such as conflict, defense mechanisms, object relations, attachment patterns, and developmental considerations. These constructs help in understanding the individual's internal world and the dynamics that influence their thoughts, feelings, and behaviors.\n",
      "\n",
      "4. Clinical utility: The OPD-2 is designed to be clinically useful for psychodynamic therapists. It provides a framework for formulating cases, understanding the patient's problems, and planning treatment interventions. The manual includes detailed guidelines, examples, and case vignettes to assist clinicians in applying the system in their practice.\n",
      "\n",
      "5. Integration with other diagnostic systems: While the OPD-2 is primarily a psychodynamic diagnostic system, it also recognizes the value of integrating with other diagnostic frameworks, such as the DSM (Diagnostic and Statistical Manual of Mental Disorders) or ICD (International Classification of Diseases). This integration allows for a more comprehensive understanding of mental disorders and facilitates communication between clinicians from different theoretical orientations.\n",
      "\n",
      "The OPD-2 is widely recognized and used in psychodynamic clinical settings and research. It provides a sophisticated and nuanced approach to understanding psychological problems and has contributed significantly to the field of psychodynamic diagnosis and treatment planning.\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dbeb38494b6b2cf8af8d01d83605d6c4', 'x-endpoint-monthly-call-limit': '1000', 'x-trial-endpoint-call-limit': '10', 'x-trial-endpoint-call-remaining': '7', 'date': 'Sat, 19 Jul 2025 16:48:40 GMT', 'content-length': '147', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 400, body: {'id': '32688157-80ab-4f4c-a47a-d84afe860088', 'message': 'invalid request: message must be at least 1 token long or tool results must be specified.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m llm = Cohere(model=\u001b[33m\"\u001b[39m\u001b[33mcommand-r-plus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m chat_engine = SimpleChatEngine.from_defaults(llm=llm)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mchat_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_repl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/core/chat_engine/types.py:339\u001b[39m, in \u001b[36mchat_repl\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.achat_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33machat_stream is None!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chat_response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.achat_stream:\n\u001b[32m    342\u001b[39m         \u001b[38;5;28mself\u001b[39m.unformatted_response += chat_response.delta \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/core/callbacks/utils.py:41\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m     40\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/core/chat_engine/simple.py:97\u001b[39m, in \u001b[36mchat\u001b[39m\u001b[34m(self, message, chat_history)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     initial_token_count = \u001b[32m0\u001b[39m\n\u001b[32m     95\u001b[39m all_messages = \u001b[38;5;28mself\u001b[39m._prefix_messages + \u001b[38;5;28mself\u001b[39m._memory.get(\n\u001b[32m     96\u001b[39m     initial_token_count=initial_token_count\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m )\n\u001b[32m     99\u001b[39m chat_response = \u001b[38;5;28mself\u001b[39m._llm.chat(all_messages)\n\u001b[32m    100\u001b[39m ai_message = chat_response.message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:172\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m model_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    158\u001b[39m dispatcher.event(\n\u001b[32m    159\u001b[39m     LLMChatStartEvent(\n\u001b[32m    160\u001b[39m         model_dict=model_dict,\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m )\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m    169\u001b[39m         EventPayload.MESSAGES: messages,\n\u001b[32m    170\u001b[39m         EventPayload.ADDITIONAL_KWARGS: kwargs,\n\u001b[32m    171\u001b[39m         EventPayload.SERIALIZED: _self.to_dict(),\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    175\u001b[39m     f_return_val = f(_self, messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/llms/cohere/base.py:149\u001b[39m, in \u001b[36mchat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_all_kwargs\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    141\u001b[39m         **\u001b[38;5;28mself\u001b[39m._model_kwargs,\n\u001b[32m    142\u001b[39m         **kwargs,\n\u001b[32m    143\u001b[39m     }\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_chat_with_tools\u001b[39m(\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    147\u001b[39m     tools: List[\u001b[33m\"\u001b[39m\u001b[33mBaseTool\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    148\u001b[39m     user_msg: Optional[Union[\u001b[38;5;28mstr\u001b[39m, ChatMessage]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     chat_history: Optional[List[ChatMessage]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    150\u001b[39m     verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    151\u001b[39m     allow_parallel_tool_calls: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    152\u001b[39m     tool_required: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    155\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prepare the chat with tools.\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m     chat_history = chat_history \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/llms/cohere/utils.py:177\u001b[39m, in \u001b[36mcompletion_with_retry\u001b[39m\u001b[34m(client, max_retries, chat, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m COHERE_TREE_SUMMARIZE_TEMPLATE = ChatPromptTemplate(\n\u001b[32m    154\u001b[39m     message_templates=[\n\u001b[32m    155\u001b[39m         TEXT_QA_SYSTEM_PROMPT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m     ]\n\u001b[32m    167\u001b[39m )\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Table context refine (based on llama_index.core.chat_prompts::CHAT_REFINE_TABLE_CONTEXT_PROMPT)\u001b[39;00m\n\u001b[32m    169\u001b[39m COHERE_REFINE_TABLE_CONTEXT_PROMPT = ChatPromptTemplate(\n\u001b[32m    170\u001b[39m     message_templates=[\n\u001b[32m    171\u001b[39m         ChatMessage(content=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{query_str}\u001b[39;00m\u001b[33m\"\u001b[39m, role=MessageRole.USER),\n\u001b[32m    172\u001b[39m         ChatMessage(content=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{existing_answer}\u001b[39;00m\u001b[33m\"\u001b[39m, role=MessageRole.ASSISTANT),\n\u001b[32m    173\u001b[39m         DocumentMessage(content=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{context_msg}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m    174\u001b[39m         ChatMessage(\n\u001b[32m    175\u001b[39m             content=(\n\u001b[32m    176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mWe have provided a table schema below. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{schema}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mWe have also provided some context information. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mGiven the context information and the table schema, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrefine the original answer to better \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33manswer the question. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mIf the context isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt useful, return the original answer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    185\u001b[39m             ),\n\u001b[32m    186\u001b[39m             role=MessageRole.USER,\n\u001b[32m    187\u001b[39m         ),\n\u001b[32m    188\u001b[39m     ]\n\u001b[32m    189\u001b[39m )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_retry_decorator\u001b[39m(max_retries: \u001b[38;5;28mint\u001b[39m) -> Callable[[Any], Any]:\n\u001b[32m    193\u001b[39m     min_seconds = \u001b[32m4\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/llama_index/llms/cohere/utils.py:170\u001b[39m, in \u001b[36m_completion_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m COHERE_TREE_SUMMARIZE_TEMPLATE = ChatPromptTemplate(\n\u001b[32m    154\u001b[39m     message_templates=[\n\u001b[32m    155\u001b[39m         TEXT_QA_SYSTEM_PROMPT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m     ]\n\u001b[32m    167\u001b[39m )\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Table context refine (based on llama_index.core.chat_prompts::CHAT_REFINE_TABLE_CONTEXT_PROMPT)\u001b[39;00m\n\u001b[32m    169\u001b[39m COHERE_REFINE_TABLE_CONTEXT_PROMPT = ChatPromptTemplate(\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     message_templates=[\n\u001b[32m    171\u001b[39m         ChatMessage(content=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{query_str}\u001b[39;00m\u001b[33m\"\u001b[39m, role=MessageRole.USER),\n\u001b[32m    172\u001b[39m         ChatMessage(content=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{existing_answer}\u001b[39;00m\u001b[33m\"\u001b[39m, role=MessageRole.ASSISTANT),\n\u001b[32m    173\u001b[39m         DocumentMessage(content=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{context_msg}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m    174\u001b[39m         ChatMessage(\n\u001b[32m    175\u001b[39m             content=(\n\u001b[32m    176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mWe have provided a table schema below. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{schema}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m---------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mWe have also provided some context information. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mGiven the context information and the table schema, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrefine the original answer to better \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33manswer the question. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mIf the context isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt useful, return the original answer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    185\u001b[39m             ),\n\u001b[32m    186\u001b[39m             role=MessageRole.USER,\n\u001b[32m    187\u001b[39m         ),\n\u001b[32m    188\u001b[39m     ]\n\u001b[32m    189\u001b[39m )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_retry_decorator\u001b[39m(max_retries: \u001b[38;5;28mint\u001b[39m) -> Callable[[Any], Any]:\n\u001b[32m    193\u001b[39m     min_seconds = \u001b[32m4\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/cohere/client.py:103\u001b[39m, in \u001b[36mexperimental_kwarg_decorator.<locals>._wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_kwarg(deprecated_kwarg, kwargs):\n\u001b[32m     99\u001b[39m     logger.warning(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_kwarg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` parameter is an experimental feature and may change in future releases.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/cohere/client.py:35\u001b[39m, in \u001b[36mvalidate_args.<locals>._wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m     34\u001b[39m     check_fn(*args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/cohere/base_client.py:710\u001b[39m, in \u001b[36mBaseCohere.chat\u001b[39m\u001b[34m(self, message, accepts, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    439\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    465\u001b[39m     request_options: typing.Optional[RequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    466\u001b[39m ) -> NonStreamedChatResponse:\n\u001b[32m    467\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[33;03m    Generates a text response to a user message.\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[33;03m    To learn how to use the Chat API and RAG follow our [Text Generation guides](https://docs.cohere.com/docs/chat-api).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    708\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m    709\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     _response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccepts\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccepts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreamble\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreamble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconversation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_truncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_truncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconnectors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43msearch_queries_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_queries_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcitation_quality\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcitation_quality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_input_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_single_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_single_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafety_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github/hands-on-ai-rag-using-llamaindex-3830207/.venv/lib/python3.12/site-packages/cohere/raw_base_client.py:838\u001b[39m, in \u001b[36mRawBaseCohere.chat\u001b[39m\u001b[34m(self, message, accepts, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[39m\n\u001b[32m    836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(response=_response, data=_data)\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _response.status_code == \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m    839\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m    840\u001b[39m         body=typing.cast(\n\u001b[32m    841\u001b[39m             typing.Optional[typing.Any],\n\u001b[32m    842\u001b[39m             construct_type(\n\u001b[32m    843\u001b[39m                 type_=typing.Optional[typing.Any],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    844\u001b[39m                 object_=_response.json(),\n\u001b[32m    845\u001b[39m             ),\n\u001b[32m    846\u001b[39m         ),\n\u001b[32m    847\u001b[39m     )\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _response.status_code == \u001b[32m401\u001b[39m:\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnauthorizedError(\n\u001b[32m    850\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m    851\u001b[39m         body=typing.cast(\n\u001b[32m   (...)\u001b[39m\u001b[32m    857\u001b[39m         ),\n\u001b[32m    858\u001b[39m     )\n",
      "\u001b[31mBadRequestError\u001b[39m: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dbeb38494b6b2cf8af8d01d83605d6c4', 'x-endpoint-monthly-call-limit': '1000', 'x-trial-endpoint-call-limit': '10', 'x-trial-endpoint-call-remaining': '7', 'date': 'Sat, 19 Jul 2025 16:48:40 GMT', 'content-length': '147', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 400, body: {'id': '32688157-80ab-4f4c-a47a-d84afe860088', 'message': 'invalid request: message must be at least 1 token long or tool results must be specified.'}"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(llm=llm)\n",
    "\n",
    "chat_engine.chat_repl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
